{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPMtcxAH0DcOQ2E47v5JpB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robertobiero/PythonMovieAnalysis/blob/master/SelfAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_iPnepooUVc",
        "outputId": "daac3493-49d8-4408-ecb9-5fbb6aa4a83e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q_journey tensor([0.4306, 1.4551])\n",
            "K_journey tensor([0.4433, 1.1419])\n",
            "V_journey tensor([0.3951, 1.0037])\n",
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
            "attention weights tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
            "Value vector tensor([[0.1855, 0.8812],\n",
            "        [0.3951, 1.0037],\n",
            "        [0.3879, 0.9831],\n",
            "        [0.2393, 0.5493],\n",
            "        [0.1492, 0.3346],\n",
            "        [0.3221, 0.7863]])\n",
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "\n",
        "# Randomly initialize W_q, W_k, W_v\n",
        "torch.manual_seed(123)\n",
        "\n",
        "x_2 = inputs[1] # second input element\n",
        "d_in = inputs.shape[1] # the input embedding size, d=3\n",
        "d_out = 2 # the output embedding size, d=2\n",
        "\n",
        "# this are the weights for all the tokens\n",
        "\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "\n",
        " # we can project all the input emeddings with the above weights to get Q, K, V\n",
        "\n",
        "Q = inputs @ W_query\n",
        "K = inputs @ W_key\n",
        "V = inputs @ W_value\n",
        "\n",
        "\n",
        "Q_journey = x_2 @ W_query\n",
        "K_journey = x_2 @ W_key\n",
        "V_journey = x_2 @ W_value\n",
        "\n",
        "# query , key and value of the input word \"journey\"\n",
        "\n",
        "print(\"Q_journey\", Q_journey)\n",
        "print(\"K_journey\", K_journey)\n",
        "print(\"V_journey\", V_journey)\n",
        "\n",
        "# to get attention scores for the word \"journey\" multiply journeys Q\n",
        "# K is a 6 * 2 vector so we need to transpose it to multiply by Q_journey\n",
        "\n",
        "journey_attention_scores = Q_journey @ K.T\n",
        "\n",
        "\n",
        "print(journey_attention_scores)\n",
        "\n",
        "d_k = K.shape[1]\n",
        "\n",
        "attn_weights_journey = torch.softmax(journey_attention_scores / d_k**0.5, dim=-1)\n",
        "\n",
        "# after passing the scores through soft max we should get a matrix that sums to 1\n",
        "# this is essentially a probabilty distribution\n",
        "\n",
        "context_vec_journey = attn_weights_journey @ V\n",
        "\n",
        "print(\"attention weights\", attn_weights_journey)\n",
        "\n",
        "print(\"Value vector\", V)\n",
        "\n",
        "print(\"Z or context vector\", context_vec_journey)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-hozSF0Qob57"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}